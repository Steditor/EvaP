\subsubsection{Test Automation}
The project is already covered by several tools.
Our research showed the used tools are popular in the Python community if the project is hosted on GitHub.
Because of this and since the developers are comfortable with their choices, we will observe the functionality of the tools during the project.
We will focus this section on describing our experience with the set up of running tests locally.

\paragraph{Django Tests}
The web framework Django used by EvaP comes with an integrated test runner embedded in Django's \texttt{manage.py} utility script.
This test runner executes python test cases inside an isolated execution environment provided by Django.

To be able to run the specified tests, one therefor has to set up EvaP as described in \autoref{setup}, access the server via a remote console via \texttt{ssh} and call \texttt{python3 manage.py test <evap module test> [<evap module test> [...]]}.
The provided console output then informs about successful and failed tests.
Exemplary usage of this utility is provided by the configuration of \textit{Travis} located in the \texttt{.travis.yml} file in the project's root.
Travis is a web service that automatically tests the code of each pull request submitted on GitHub.

By these means it is enforced, that all existing tests are run at least once before code changes are merged into the production source code.

\paragraph{PyCharm integration}
For local development we decided to code and test with PyCharm, a professional IDE for python, that also claims to support Django and Vagrant out of the box.
Since the recently published release of PyCharm version 5, this support almost covers all configuration issues of remote development.
Whilst previous versions did not configure the ssh access to the virtual execution environment correctly, the developer now only has to run the automatic project configuration of PyCharm for Django Vagrant projects and then add a path mapping of the local source code to the path of the corresponding source files inside the virtual machine to be able to execute tests.
The test integration then provides an intuitive graphical overview of the status of all test cases, and can even be configured to rerun automatically on code changes.
However, as the execution of the whole test set currently takes a few minutes, it is not feasible to constantly run the whole test set during development.

\paragraph{Code coverage measurement}
Code coverage of python projects can be measured with the coverage package%
\footnote{\url{https://pypi.python.org/pypi/coverage}}.
This package records statement coverage and can be configured to measure branch coverage as well.
According to our research, there currently does not seem to be a tool that implements further coverage criteria such as logic coverage.
To run the tests with coverage, one has to call the utility script with \texttt{coverage run}.
From the collected coverage data one can generate a HTML report or convert the data in a xml-based format that can e.g. be read by PyCharm.
In theory PyCharm integrates coverage collection, in combination with Vagrant there still seems to be a bug that always leads to a displayed coverage of 0\%.
If one however imports a generated xml report, PyCharm enriches its editor by displaying the current line coverage along the source code.
EvaP uses an integration of the web service \textit{Coveralls} into the Travis build process that roughly provides the same view as PyCharm.
